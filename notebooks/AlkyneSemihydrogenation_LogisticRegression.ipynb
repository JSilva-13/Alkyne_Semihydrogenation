{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a48e5049",
   "metadata": {},
   "source": [
    "# Jupyter Notebook for the Data Analysis of Alkyne Semihydrogenation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57043a15",
   "metadata": {},
   "source": [
    "This notebook is inspired/based on work and analyses done in:\n",
    "\n",
    "* S. H. Newman-Stonebraker, A. G. Doyle *et al.*, *Science* **2021**, *374*, 301-308. Jupyter notebook available under: https://github.com/SigmanGroup/Threshold  \n",
    "* J. Dotson, M. Sigman *et al.*, *J. Am. Chem. Soc.* **2023**, *145, 1*, 110-121. Jupyter notebook available under: https://github.com/SigmanGroup/Multiobjective_Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450cc03c",
   "metadata": {},
   "source": [
    "# 1. Single-Node Decision Tree (Threshold Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcf3267",
   "metadata": {},
   "source": [
    "## Reading data\n",
    "### Choosing input files\n",
    "\n",
    "- Sets up script to import data and parameters from excel\n",
    "- populate this cell based on the excel spreadsheet that you want to be read\n",
    "- populate y_cut with threshold value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c89494",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
    "from sklearn import metrics\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_file = 'DataSet_AllAlkynes_TMS' # parameter excel file name \n",
    "comp_sheet = \"DataSheet\" # parameter excel sheet name\n",
    "num_par = 44 # number of parameters\n",
    "par_start_col = 3  # 0-indexed, first column number with descriptor values\n",
    "comp_num_samples = 2716 # Number of substrates in sheet\n",
    "y_label_col_comp = 0 # 0-indexed, the number of the column with the substrate ids\n",
    "\n",
    "exp_file = 'DataSet_Tested' # data excel file name\n",
    "exp_sheet = 'DataSheet' # data excel sheet name\n",
    "exp_num_samples = 31 # include all the ligands, the empty rows will be removed later\n",
    "response_col = 2 # 0-indexed, the column containing the experimental results\n",
    "y_label_col_exp= 1 # 0-indexed, the number of the column with the substrate ids\n",
    "\n",
    "y_cut = 0.033   # this sets the threshold value for 1/0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78bb89e",
   "metadata": {},
   "source": [
    "### Sorting data\n",
    "\n",
    "- Imports parameters and data and converts continuous data to binary data\n",
    "- No user input required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a91ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************\n",
    "#Creation of dataframe, extraction of descriptors and output variables\n",
    "#********************************************************************\n",
    "\n",
    "# make pd df for the descriptors\n",
    "compinp = pd.read_excel(comp_file+\".xlsx\",comp_sheet,header=0,index_col=y_label_col_comp,nrows=comp_num_samples+1,usecols=list(range(0,(num_par+par_start_col))),engine='openpyxl')\n",
    "compinp = compinp.drop(['SMILES'],axis=1)\n",
    "\n",
    "compinp.dropna(inplace=True)\n",
    "par_start_col = 1\n",
    "compinp.index = compinp.index.map(str)\n",
    "\n",
    "# make pd df for the experimental file\n",
    "expinp = pd.read_excel(exp_file+\".xlsx\",exp_sheet,header=2,index_col=y_label_col_exp,nrows=exp_num_samples,usecols=list(range(0,response_col+1)),engine='openpyxl')\n",
    "\n",
    "# names of the descriptors\n",
    "X_names = list(compinp.iloc[0,par_start_col-1:num_par+par_start_col-1])\n",
    "\n",
    "# labels for descriptors e.g. x1, x2, x3... make list then take only selection for descriptors\n",
    "X_labels = list(compinp.columns)[par_start_col-1:num_par+par_start_col-1]\n",
    "\n",
    "# removes names of the descriptors from compinp\n",
    "compinp.drop(index=compinp.index[0],inplace=True)\n",
    "\n",
    "# X_all = np array of descriptor values for all substrates\n",
    "X_all = np.asarray(compinp[X_labels],dtype=np.float)\n",
    "\n",
    "# np array of the alkyne ids from the descriptor file\n",
    "y_labels_comp = np.asarray(list(compinp.index),dtype=str)\n",
    "\n",
    "# compnan = array of True/False designating if a substrate has a missing descriptor value(s) or not\n",
    "# nan = not a number. isnan returns True if Nan, in this case for any value in a row (axis of 1 = row).\n",
    "compnan = np.isnan(X_all).any(axis=1)\n",
    "\n",
    "# compares the arrays, and keeps the sample in y_labels_comp/X_all if the corresponding value in ~compnan = True.\n",
    "# ~ means it inverts True and False in compnan. This is removing any substrate missing descriptors.\n",
    "y_labels_comp,X_all = y_labels_comp[~compnan],X_all[~compnan]\n",
    "\n",
    "# combines the labels and names of descriptors as a single value in a list e.g. \"x1 E_HOMO\" \n",
    "X_labelname = [\" \".join(i) for i in zip(X_labels,X_names)]\n",
    "\n",
    "# makes a dictionary with key of descriptor label, value of descriptor name\n",
    "X_labelname_dict = dict(zip(X_labels,X_names))\n",
    "\n",
    "# heading ('label') for the response column\n",
    "resp_label = list(expinp.columns)[response_col-1]\n",
    "\n",
    "# array of the experimental results\n",
    "y = np.asarray(expinp.iloc[:,response_col-1],dtype=np.float)\n",
    "\n",
    "# array of all the ligand ids in the experimental file (curated below to give ligands with results only)\n",
    "y_labels_exp = np.asarray(list(expinp.index),dtype=str)\n",
    "\n",
    "# array with True for experimental results present, False for none\n",
    "mask_y = ~np.isnan(y)\n",
    "\n",
    "# check if each value of y_labels_exp (ligand ids in exp file) is also in y_labels_comp (ligand ids in descriptor file),\n",
    "# to give an array with True/False if a match is found (i.e. do we have the descriptors we need) \n",
    "mask_X = np.array([True if i in y_labels_comp else False for i in y_labels_exp])\n",
    "\n",
    "# compares the two arrays, if same value is True in both then value = True in mask, otherwise = False.\n",
    "# i.e. does the ligand have an experimental result and descriptors\n",
    "mask = mask_y&mask_X\n",
    "\n",
    "# ligands_removed is a list of ligands that had zero-values\n",
    "count = 0\n",
    "ligands_removed = []\n",
    "for i in list(mask):\n",
    "    if not i:\n",
    "        ligands_removed.append(y_labels_exp[count])\n",
    "    count += 1\n",
    "\n",
    "print(\"Number of entries in experimental file before removing empty cells: {}\".format(len(y)))\n",
    "print(\"Removing {} entries with empty cells\".format(len(y)-sum(mask)))\n",
    "print('Entries removed: ', ligands_removed)\n",
    "\n",
    "# remove all nan values from y, leaving experimental results\n",
    "y = y[np.array(mask)]\n",
    "\n",
    "# Convert reaction data to binary 1/0 based on y_cut\n",
    "y_class = np.array([1 if result > y_cut else 0 for result in y])\n",
    "\n",
    "# cut y_labels to only have ids for alkynes with results\n",
    "y_labels = y_labels_exp[np.array(mask)]\n",
    "\n",
    "# X = array of descriptor values for the alkynes with experimental results\n",
    "X = np.asarray(compinp.loc[y_labels],dtype=np.float)\n",
    "\n",
    "\n",
    "\n",
    "verbose = True\n",
    "if verbose:\n",
    "    print(\"Shape of descriptors file for all alkynes: {}\".format(X_all.shape))\n",
    "    print(\"Last three ids in the descriptor file: {}\".format(y_labels_comp[-3:]))\n",
    "    print(\"Shape of descriptors file for alkynes with experimental results: {}\".format(X.shape))\n",
    "    print(\"Shape of results file results (only alkynes with experimental results): {}\".format(y.shape)) \n",
    "    print(\"Shape of results file labels (only alkynes with experimental results): {}\".format(y_labels.shape))\n",
    "    print(\"First descriptor cell (for alkynes with experimental results): {}\".format(X[0,0]))\n",
    "    print(\"Last descriptor cell (for alkynes with experimental results):  {}\".format(X[-1,-1]))\n",
    "    print('Alkynes with results:',y_labels)\n",
    "    print(len(y),' Experimental results:', \" : \", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c889a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#********************************************************************\n",
    "#Single Node Decision Tree Analysis for all Features\n",
    "#********************************************************************\n",
    "\n",
    "#class_weight = {0:1,1:20}   # define class weights\n",
    "class_weight = \"balanced\"\n",
    "accuracy_cutoff = 0.9 #define accuracy cutoff to only get selected features\n",
    "\n",
    "target_names = ['0 (\"negatives\")',  '1 (\"positives\")']\n",
    "\n",
    "#set which parameters/features to iterate through\n",
    "features = range(0,len(X_labels))\n",
    "\n",
    "#threshold analysis for selected features\n",
    "for f_ind in features:\n",
    "    feature = X_labels[f_ind] \n",
    "    \n",
    "    dt = DecisionTreeClassifier(max_depth=1,class_weight=class_weight).fit(X[:,f_ind].reshape(-1, 1), y_class)  \n",
    "    if dt.score(X[:,f_ind].reshape(-1, 1), y_class) >= accuracy_cutoff:\n",
    "        print(\"N = {}\\nFeature: {}\\nDecision threshold = {:.2f}\\nAccuracy: {:.2f}\\nf1_score: {:.2f}\\nMCC: {:.2f} \\nRecall: = {:.2f}\\nClassification Report: \\n{}\".format(len(y),feature,\n",
    "        dt.tree_.threshold[0],\n",
    "        dt.score(X[:,f_ind].reshape(-1, 1), y_class),\n",
    "        metrics.f1_score(y_class,dt.predict(X[:,f_ind].reshape(-1, 1))),\n",
    "        metrics.matthews_corrcoef(y_class,dt.predict(X[:,f_ind].reshape(-1, 1))),\n",
    "        metrics.recall_score(y_class, dt.predict(X[:,f_ind].reshape(-1, 1)), pos_label=1, average='binary'),\n",
    "        metrics.classification_report(y_class, dt.predict(X[:,f_ind].reshape(-1, 1)), target_names=target_names),              \n",
    "        ))\n",
    "    \n",
    "        # begin plot\n",
    "        dt_plt = DecisionTreeClassifier(max_depth=1,class_weight=class_weight).fit(X[:,f_ind].reshape(-1, 1), y_class)\n",
    "        n_classes = 2\n",
    "        plot_step = 0.02\n",
    "    \n",
    "        # define plot axes limits here as appropriate \n",
    "        x_min, x_max = X[:,f_ind].min(), X[:,f_ind].max()\n",
    "        y_min, y_max = y.min(), y.max()\n",
    "    \n",
    "        # set plot colors\n",
    "        cMap_background = ListedColormap(['white', '#ccebc5',]) # color for the backgrounds: light green and red\n",
    "        cMap_points = ListedColormap([\"r\",\"g\"]) # the color for each class of the actual data points (i.e. inactive/active)). \"rg\" is red,green\n",
    "    \n",
    "        # plot code\n",
    "        fig = plt.figure(figsize=(6, 6)) \n",
    "        ax = fig.add_subplot()\n",
    "        for axis in ['top','bottom','left','right']:\n",
    "            ax.spines[axis].set_linewidth(3)\n",
    "        ax.tick_params(length=12,width=3)\n",
    "        ax.set_ylim([y_min-0.05*y_max,y_max*1.1])\n",
    "        ax.set_xlim([x_min-0.05*x_max,x_max*1.1])\n",
    "\n",
    "        plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=3.5)\n",
    "        plt.xlabel(feature,fontsize=18)\n",
    "        plt.ylabel(r\"mmol $H_{2}$ / h\",fontsize=18)\n",
    "        plt.xticks(fontsize=15)\n",
    "        plt.yticks(fontsize=15)\n",
    "\n",
    "        ax.axhspan(ymin=y_min-0.05*y_max,ymax=y_cut,color='grey',alpha=0.15)\n",
    "        ax.axvspan(xmin=x_min-0.05*x_max,xmax=dt.tree_.threshold[0],color=\"grey\",alpha=0.15)\n",
    "    \n",
    "        plt.scatter(X[:,f_ind],y,c=y_class,cmap=cMap_points,edgecolor=\"black\",s=75)\n",
    "    \n",
    "        # Print plot\n",
    "        plt.show()\n",
    "    \n",
    "        print('-------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69061f6b",
   "metadata": {},
   "source": [
    "# 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f02ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split,RepeatedKFold,cross_val_score,cross_validate\n",
    "from logreg_stats import calc_mcfad, calc_mcfadden_R2, precision_recall_f1_score, test_accuracy_score, kfold_logreg \n",
    "import multiprocessing\n",
    "nproc = max([1,multiprocessing.cpu_count()-2])\n",
    "\n",
    "randomstate = 42\n",
    "import Logistic_Regression as fsc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda63fe8",
   "metadata": {},
   "source": [
    "## 2.1. Define Plotting Functions\n",
    "### Define plotting function for one-parameter logistic regression\n",
    "\n",
    "- No user input required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a17c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_fit_1D(feat, df_combined, save_fig=False):  \n",
    "    X_train = np.array(df_combined.loc[:, [feat]])\n",
    "    y_train = np.array(df_combined.iloc[:, -1])\n",
    "    lr = LogisticRegression().fit(X_train,y_train)\n",
    "    m, b = lr.coef_[0][0], lr.intercept_[0]\n",
    "    feat_name = X_labelname_dict[feat]\n",
    "\n",
    "    x_min, x_max = min(X_train), max(X_train)\n",
    "    x_range = x_max - x_min\n",
    "    plot_min, plot_max = float(x_min-0.05*x_range), float(x_max+0.05*x_range)\n",
    "\n",
    "    fig= plt.figure(figsize=(6,6))\n",
    "    ax = fig.add_subplot()\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(3)\n",
    "    ax.tick_params(length=12,width=3)\n",
    "\n",
    "    ax.tick_params(which='minor', length=6, width=3)\n",
    "\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=3.5)\n",
    "\n",
    "    plt.xticks(fontsize=15) \n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel(feat_name,fontsize=18)\n",
    "    plt.ylabel('Probability',fontsize=18)\n",
    "    #plt.locator_params(axis='y', nbins=4)\n",
    "    #plt.locator_params(axis='x', nbins=5)\n",
    "    plt.axvline(x=(-b/m),alpha=1,c='black')\n",
    "    plt.axvline(x=((np.log(3)-b)/m),alpha=1,c='black', linestyle = '--')\n",
    "    plt.axvline(x=((np.log(1/3)-b)/m),alpha=1,c='black', linestyle = '--')\n",
    "    plt.scatter(X_train, y_train, label=\"training\", alpha=1,marker='o', c ='Blue', s=150  ,edgecolor='black')\n",
    "    plt.xlim(plot_min, plot_max)\n",
    "    plt.ylim(-.05, 1.05)\n",
    "\n",
    "    x = np.linspace(x_min-0.4*x_range, x_max+0.4*x_range)\n",
    "    f_x = np.exp(b + m*x)\n",
    "    y_sigmoid = f_x/(f_x + 1)\n",
    "    plt.plot(x, y_sigmoid, color = 'black')\n",
    "    if save_fig:\n",
    "        fig.savefig(feat + '.tif', dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61bd8a4",
   "metadata": {},
   "source": [
    "## 2.2. Univariate correlations and feature/feature plotting\n",
    "\n",
    "### Univariate Logistic Regression\n",
    "\n",
    "- Runs univariate logistic regression for all parameters and creates a dataframe of models in decending order of McFadden R2\n",
    "- No user input required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04981065",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1_param = pd.DataFrame(columns=['Model', 'Accuracy', 'McFadden_R2', 'Param_name', 'Threshold_Value'])\n",
    "\n",
    "count = 0\n",
    "for i in range(len(X_labels)):\n",
    "    term = X_labels[i]\n",
    "    X_sel = X[:, i].reshape(-1,1)\n",
    "    lr = LogisticRegression().fit(X_sel,y_class)\n",
    "    acc = round(lr.score(X_sel,y_class), 2)\n",
    "    mcfad_r2 = round(calc_mcfad(X_sel, y_class), 2)\n",
    "    m, b = lr.coef_[0][0], lr.intercept_[0]\n",
    "    row_i = {'Model': term, 'Accuracy': acc, 'McFadden_R2': mcfad_r2, 'Param_name': X_labelname_dict[term], 'Threshold_Value': -b/m}\n",
    "    results_1_param = results_1_param.append(row_i, ignore_index=True)\n",
    "\n",
    "results_1_param = results_1_param.sort_values('McFadden_R2', ascending=False)\n",
    "results_1_param.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e3eb71",
   "metadata": {},
   "source": [
    "### Plot top univariates\n",
    "\n",
    "- prints top n models (n = num_plots)\n",
    "- populate num_plots and skipfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfc2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_plots = 3  #Specify how many univariates to plot\n",
    "skipfeatures = []  # add any features that you don't want to plot, e.g. ['x1', 'x28']\n",
    "\n",
    "\n",
    "df_combined = pd.DataFrame(np.hstack((X,y_class[:,None]))) \n",
    "newcols = [\"x\"+str(i+1) for i in df_combined.columns.values]\n",
    "df_combined.columns = newcols\n",
    "response = newcols[-1]\n",
    "df_combined.rename(columns={response:\"y\"},inplace=True)\n",
    "df_combined.drop(skipfeatures,axis=1,inplace=True)\n",
    "\n",
    "for i in range(num_plots):\n",
    "    param_i = results_1_param.iloc[i].Model\n",
    "    param_name = X_labelname_dict[param_i]\n",
    "    threshold = results_1_param.iloc[i].Threshold_Value\n",
    "    print(\"{} {}. Threshold value {:.2f}\".format(param_i, param_name, threshold))\n",
    "    print(\"Accuracy: {:.0f}%\".format(100*results_1_param.iloc[i].Accuracy))\n",
    "    print(\"McFadden R2: {}\".format(results_1_param.iloc[i].McFadden_R2))\n",
    "    plot_fit_1D(param_i, df_combined, save_fig=False)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d866b8e",
   "metadata": {},
   "source": [
    "### Plot a univariate logistic regression with a user-defined parameter \n",
    "\n",
    "- Runs univariate logistic regression for user-defined parameter list\n",
    "- Populate plot_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d6b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_params = ['x28','x41'] #populate with a list of parameters (e.g. plot_params = ['x1', 'x20', 'x21'])\n",
    "\n",
    "for param_i in plot_params:\n",
    "    row = results_1_param[results_1_param.Model == param_i]\n",
    "    param_name = X_labelname_dict[param_i]\n",
    "    threshold = float(row.Threshold_Value)\n",
    "    acc = float(row.Accuracy)\n",
    "    mcfad_r2 = float(row.McFadden_R2)\n",
    "    print(\"{} {}. Threshold value {:.2f}\".format(param_i, param_name, threshold))\n",
    "    print(\"Accuracy: {:.0f}%\".format(100*acc))\n",
    "    print(\"McFadden R2: {}\".format(mcfad_r2))\n",
    "    plot_fit_1D(param_i,df_combined)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5b55e1",
   "metadata": {},
   "source": [
    "### Feature vs. another feature\n",
    "\n",
    "- plots one feature vs another to determine possible correlations\n",
    "- Select two features to visualize (f_ind_1, f_ind_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be9847",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ind_1 = \"x28\" # feature of interest 1 (e.g. 'x1')\n",
    "f_ind_2 = \"x9\" # feature of interest 2 (e.g. 'x20')\n",
    "\n",
    "if type(f_ind_1) == str:\n",
    "    [f_ind_1,f_ind_2] = [X_labels.index(i) for i in [f_ind_1,f_ind_2]]\n",
    "\n",
    "print(X_labels[f_ind_1], X_names[f_ind_1])\n",
    "print(X_labels[f_ind_2], X_names[f_ind_2])\n",
    "print(\"\\n{} samples\".format(np.shape(X[:,f_ind_1])[0]))\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(X[:,f_ind_1],X[:,f_ind_2])\n",
    "fit_line = intercept+slope*X[:,f_ind_1]\n",
    "print(\"R^2 = {:.2f}; p-value = {:.2E}\".format(r_value**2,p_value))\n",
    "\n",
    "plt.figure(figsize=(13, 4))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.hist(X[:,f_ind_1], bins=\"auto\",color=\"black\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.xlabel(X_names[f_ind_1])\n",
    "plt.subplot(1,3,2)\n",
    "plt.hist(X[:,f_ind_2], bins=\"auto\",color=\"black\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.xlabel(X_names[f_ind_2])\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(X[:,f_ind_1], X[:,f_ind_2],color=\"black\",marker=\".\",alpha=0.5,s=100)    \n",
    "plt.plot(X[:,f_ind_1],fit_line)\n",
    "plt.xlabel(X_names[f_ind_1])\n",
    "plt.ylabel(X_names[f_ind_2])\n",
    "plt.tight_layout()\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf68ba",
   "metadata": {},
   "source": [
    "# 3. Multivariate Logisitc Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9caf46",
   "metadata": {},
   "source": [
    "### Define plotting function for 2D logistic regression\n",
    "\n",
    "- No user input required\n",
    "- colormap_scheme, test_points, train_points, point_color, and point_size can be changed to adjust the appearance of the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753d33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap_scheme = 'RdBu'\n",
    "test_points, train_points = 'X', 'o' # marker shape of test set ('*', <, >, x, X, o, O, b, d,. )\n",
    "point_color = 'Greys'# 'Greens', 'Oranges', 'Reds','YlOrBr', 'YlOrRd', 'OrRd', 'PuRd', 'RdPu', 'BuPu', 'GnBu', 'PuBu', 'YlGnBu', 'PuBuGn', 'BuGn', 'YlGn'\n",
    "point_size = 100\n",
    "\n",
    "def heatmap_logreg(feat1, feat2, df_train, df_test, model, annotate_test = False, annotate_train = False):  \n",
    "    #Train model on features\n",
    "    p1_vals = df_train.iloc[:, feat1]\n",
    "    p2_vals = df_train.iloc[:, feat2]\n",
    "    p1_vals_test = df_test.iloc[:, feat1]\n",
    "    p2_vals_test = df_test.iloc[:, feat2]\n",
    "    lr = LogisticRegression().fit(df_train.iloc[:,[feat1, feat2]],y_train)\n",
    "    intercept = lr.intercept_[0]\n",
    "    c1, c2 = lr.coef_[0][0], lr.coef_[0][1]\n",
    "    p1, p2 = X_names[feat1], X_names[feat2]\n",
    "    \n",
    "    if p1==\"δ_Cavg\":\n",
    "        p1= r\"$\\delta_{\\mathregular{C}}^{\\mathregular{{avg}}}$\"\n",
    "    if p2==\"δ_Cavg\":\n",
    "        p2= r\"$\\delta_{\\mathregular{C}}^{\\mathregular{{avg}}}$\"\n",
    "    if p1==\"NBO_BDE -2\":\n",
    "        p1= r\"NBO_BD$_{\\mathregular{E-2}}$\"\n",
    "    if p2==\"NBO_BDE -2\":\n",
    "        p2= r\"NBO_BD$_{\\mathregular{E-2}}$\"\n",
    "        \n",
    "    #Get max/min values for X and Y axes\n",
    "    max_x, min_x = max(list(p1_vals) + list(p1_vals_test)), min(list(p1_vals) + list(p1_vals_test))\n",
    "    max_y, min_y = max(list(p2_vals) + list(p2_vals_test)), min(list(p2_vals) + list(p2_vals_test))\n",
    "    range_x, range_y = abs(max_x - min_x), abs(max_y - min_y)\n",
    "    max_x_plt, min_x_plt = max_x + 0.1*range_x, min_x - 0.1*range_x\n",
    "    max_y_plt, min_y_plt = max_y + 0.1*range_y, min_y - 0.1*range_y\n",
    "    \n",
    "    #heatmap code\n",
    "    xx = np.linspace(min_x_plt, max_x_plt, 500)\n",
    "    yy = np.linspace(min_y_plt, max_y_plt, 500)\n",
    "    xx,yy = np.meshgrid(xx, yy)\n",
    "    Xfull = np.c_[xx.ravel(), yy.ravel()]\n",
    "   \n",
    "    # Predict probabilities of full grid\n",
    "    probas = lr.predict_proba(Xfull)\n",
    "    \n",
    "\n",
    "    fig= plt.figure(figsize=(7.25,6))\n",
    "    ax = fig.add_subplot()\n",
    "    for axis in ['top','bottom','left','right']:\n",
    "        ax.spines[axis].set_linewidth(3)\n",
    "    ax.tick_params(length=12,width=3)\n",
    "\n",
    "    ax.tick_params(which='minor', length=6, width=3)\n",
    "\n",
    "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=3.5)\n",
    "    plt.xticks(fontsize=15) \n",
    "    plt.yticks(fontsize=15)\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(4))\n",
    "    plt.xlabel(p1,fontsize=18)\n",
    "    plt.ylabel(p2,fontsize=18)\n",
    "\n",
    "    \n",
    "    x = np.linspace(min_x - 0.1*range_x, max_x + 0.1*range_x)\n",
    "    y = -(intercept/c2) - (c1/c2)*x\n",
    "    y_75 = ((np.log(3)-intercept)/c2) - (c1/c2)*x    #\n",
    "    y_25 = ((np.log(1/3)-intercept)/c2) - (c1/c2)*x\n",
    "    \n",
    "    plt.xlim([min_x - 0.1*range_x, max_x + 0.1*range_x])\n",
    "    plt.ylim([min_y - 0.1*range_y, max_y + 0.1*range_y])\n",
    "\n",
    "    plt.plot(x, y, color = 'black')\n",
    "    plt.plot(x,y_75, color = 'black', linestyle = '--')\n",
    "    plt.plot(x,y_25, color = 'black', linestyle = '--')\n",
    "    \n",
    "    heatmap = plt.imshow(\n",
    "            probas[:,1].reshape((500, 500)), cmap=colormap_scheme, alpha = 0.5, extent=[min_x_plt, max_x_plt, min_y_plt, max_y_plt],interpolation='nearest', origin=\"lower\"\n",
    "        ,aspect='auto')\n",
    "    cbar = plt.colorbar(heatmap)\n",
    "    cbar.ax.tick_params(labelsize=15)\n",
    "    cbar.set_label('Probability', size=18)\n",
    "    plt.clim(0,1)\n",
    "    plt.locator_params(axis='y', nbins=5)\n",
    "    plt.locator_params(axis='x', nbins=5)\n",
    "    plt.scatter(p1_vals, p2_vals, label=\"training\", alpha=1,marker='o', c = df_train.iloc[:, -1], cmap=point_color + '_r', s=200  ,edgecolor='black')\n",
    "    plt.scatter(p1_vals_test, p2_vals_test, label=\"test\", alpha=1,marker='X' , c = df_test.iloc[:, -1], cmap=point_color + '_r', s=200  ,edgecolor='black')\n",
    "    #plt.colorbar()\n",
    "    #cbar.set_label('yield %',rotation=90,size=22,labelpad=20)\n",
    "\n",
    "    \n",
    "    if annotate_test == True:\n",
    "        for i, txt in enumerate(VS):\n",
    "            label_i = y_labels[txt]\n",
    "            plt.annotate(label_i, (p1_vals_test[i], p2_vals_test[i]), fontsize='14', c = 'white')\n",
    "    if annotate_train == True:\n",
    "        for i, txt in enumerate(TS):\n",
    "            label_i = y_labels[txt]\n",
    "            plt.annotate(label_i, (p1_vals[i], p2_vals[i]), fontsize='14', c = 'white')\n",
    "    plt.tight_layout()\n",
    "    fig.savefig('Logistic_Regression.tif', dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771ed04b",
   "metadata": {},
   "source": [
    "## 3.1. Train-Test Spit Approaches\n",
    "\n",
    " - populate split and test_ratio: For large data set (n>100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f6bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to split the data:\n",
    "# 'random': randomly assigns train/test split\n",
    "# 'define': give a list of ligand IDs for the training set (TS) and validation set (VS) in the corresponding code section.\n",
    "# 'none': all samples in TS.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "split = \"none\"  # use one of the methods outlined above\n",
    "test_ratio = 0.3 # float from 0.0 to 1.0. Only relevant when split = 'random'\n",
    "\n",
    "X_sel,y_sel,labels_sel,exclude = X,y_class,y_labels,[]\n",
    "\n",
    "if split == \"random\":\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_sel, y_sel, random_state=randomstate+3, test_size=test_ratio)    \n",
    "    TS = [np.argwhere(np.all(X==i,axis=1))[0,0] for i in X_train]\n",
    "    VS = [np.argwhere(np.all(X==i,axis=1))[0,0] for i in X_test]\n",
    "    \n",
    "elif split == \"define\":\n",
    "    VS = [16,27,25,5,13,9,30,7,10,19]\n",
    "    TS = [24,6,22,23,18,17,2,12,20,26,8,14,1,15,4,29,21,0,28,3,11]\n",
    "    \n",
    "    X_train, y_train,X_test, y_test = X_sel[TS],y_sel[TS],X_sel[VS],y_sel[VS]\n",
    "    X_train = scaler.fit_transform(X_train)  # fit on training data\n",
    "    X_test = scaler.transform(X_test)        # apply same transformation to test data\n",
    "\n",
    "elif split == \"none\":\n",
    "    TS, VS = [i for i in range(X_sel.shape[0]) if i not in exclude],[]\n",
    "    X_train, y_train,X_test, y_test = X_sel[TS],y_sel[TS],X_sel[VS],y_sel[VS]\n",
    "    X_train = scaler.fit_transform(X_train)  # fit on training data\n",
    "else: \n",
    "    raise ValueError(\"split option not recognized\")\n",
    "\n",
    "print(\"TS: {}\".format(TS))\n",
    "print(\"VS: {}\".format(VS))\n",
    "print(\"y_mean TS: {:.3f}\".format(np.mean(y_train)))\n",
    "print(\"y_mean VS: {:.3f}\".format(np.mean(y_test)))\n",
    "print(\"Shape X_train: {}\".format(X_train.shape))\n",
    "print(\"Shape X_test:  {}\".format(X_test.shape))   \n",
    "plt.figure(figsize=(6, 6))\n",
    "hist,bins = np.histogram(y_sel,bins=\"auto\")#\"auto\"\n",
    "plt.hist(y_train, bins, alpha=0.5, label='y_train',color=\"black\")\n",
    "plt.hist(y_test, bins, alpha=0.5, label='y_test')\n",
    "plt.legend(loc='best',fontsize=14)\n",
    "plt.xlabel(\"yield\",fontsize=18)\n",
    "plt.ylabel(\"N samples\",fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755cea12",
   "metadata": {},
   "source": [
    "### Prep data\n",
    "\n",
    "- Prepares data for modelling (descriptors and results)\n",
    "- populate skipfeatures with parameters that you don't want the model to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33306c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "skipfeatures = []       \n",
    "df_train = pd.DataFrame(np.hstack((X_train,y_train[:,None])))     \n",
    "newcols = [\"x\"+str(i+1) for i in df_train.columns.values]\n",
    "df_train.columns = newcols\n",
    "response = newcols[-1]\n",
    "df_train.rename(columns={response:\"y\"},inplace=True)\n",
    "df_train.drop(skipfeatures,axis=1,inplace=True)\n",
    "\n",
    "\n",
    "df_test = pd.DataFrame(np.hstack((X_test,y_test[:,None])))\n",
    "newcols = [\"x\"+str(i+1) for i in df_test.columns.values]\n",
    "df_test.columns = newcols\n",
    "response = newcols[-1]\n",
    "df_test.rename(columns={response:\"y\"},inplace=True)\n",
    "df_test.drop(skipfeatures,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef2e068",
   "metadata": {},
   "source": [
    "###  Forward Stepwise Logistic Regression\n",
    "\n",
    "- Runs logistic regression using all 2-parameter combinations (within collin_criteria)\n",
    "- Populate collin_criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543eb12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collin_criteria = 0.5   # the maximum allowed R2 between terms in a given model\n",
    "\n",
    "# Runs forward stepwise model search\n",
    "results,models,scores,sortedmodels,candidates = fsc.ForwardStep_py(df_train,'y',\n",
    "                    collin_criteria=collin_criteria)\n",
    "\n",
    "# Identifies model with highest Accuracy\n",
    "model_sel = results.loc[0,\"Model\"]\n",
    "selected_feats = [X_labels.index(i) for i in models[model_sel].terms]\n",
    "print(\"\\n\\nModel with the highest Accuracy:\")\n",
    "print(models[model_sel].formula)\n",
    "print(\"1 + \"+\" + \".join([X_names[X_labels.index(i)] for i in models[candidates[0]].terms])+\"\\n\")\n",
    "\n",
    "# Finds features used in best model for the training/test data, reruns lr, get statistics\n",
    "X_train_sel = X_train[:,selected_feats]\n",
    "X_test_sel = X_test[:,selected_feats]\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_sel,y_train)\n",
    "kfold_score, kfold_stdev = kfold_logreg(X_train_sel, y_train,k=5)\n",
    "precision, recall, f1 = precision_recall_f1_score(X_train_sel, y_train)\n",
    "train_R2, test_R2 = calc_mcfadden_R2(X_train_sel, y_train, X_test_sel, y_test)\n",
    "\n",
    "# Plots model and prints stats\n",
    "print(\"Parameters and coefficients:\\n{:10.4f} + \\n\".format(lr.intercept_[0]) + \"\\n\".join([\"{:10.4f} * {}\".format(lr.coef_[0][i],X_labelname[selected_feats[i]]) for i in range(len(selected_feats))]))\n",
    "print(f\"\\nMcFadden Training R2  = {train_R2 :.2f}\")\n",
    "if len(VS) > 0:\n",
    "    print(f\"McFadden Test R2  = {test_R2 :.2f}\")\n",
    "print(f\"\\nTraining Accuracy   = {100 * lr.score(X_train_sel, y_train):.3f} %\")\n",
    "if len(VS) > 0:  # This is False if no data was left out as a test set. \n",
    "    print(f\"Test Accuracy  = {100 * lr.score(X_test_sel, y_test):.1f}%\")\n",
    "print(\"\\nTraining K-fold Accuracy = {:.2f} (+/- {:.2f}) %\".format(100* kfold_score, 100* kfold_stdev ** 2))\n",
    "print(f\"f1 Score  = {f1:.3f}\")\n",
    "print(f\"Precision Score  = {precision :.3f}\")\n",
    "print(f\"Recall Score  = {recall:.3f}\")\n",
    "\n",
    "if len(model_sel) > 1:\n",
    "    heatmap_logreg(selected_feats[0],selected_feats[1],df_train,df_test,results.iloc[0][0],scaler)\n",
    "else:\n",
    "    print(\"The model with the highest accuracy is a 1-parameter model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad06ce8",
   "metadata": {},
   "source": [
    "### Display multiple models\n",
    "\n",
    "- prints out the top n models in a dataframe\n",
    "- populate within the results1.head() parenthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca20a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Param_1'] = results.Model.apply(lambda x: X_labelname_dict[x[0]])\n",
    "results['Param_2'] = results.Model.apply(lambda x: X_labelname_dict[x[1]] if len(x) == 2 else \"1-term\")\n",
    "results1 = results.sort_values('McFadden_R2', ascending=False)\n",
    "results1 = results1.round(2)\n",
    "results1 = results1[['Model', 'Accuracy', 'McFadden_R2', 'Param_1', 'Param_2', 'n_terms']]\n",
    "\n",
    "results1.head(10) # put the number of models you want displayed in the parenthesis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e1a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell will print all models with a given parameter\n",
    "\n",
    "interest_param = 'δ_Cavg'\n",
    "results2 = results1[(results1.Param_1 == interest_param)| (results1.Param_2 == interest_param)]\n",
    "results2.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137ecee6",
   "metadata": {},
   "source": [
    "### Visualize a specific model\n",
    "\n",
    "- Prints plot and statistics for a model of interest\n",
    "- populate model number with the index of the model that you want to plot\n",
    "- populate annotate_train and annotate test\n",
    "\n",
    "(note that this only plots models that are within the results1 dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c82e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_number = 3\n",
    "annotate_test_set = True    # set to True if you want test set points labeled with ligand ID\n",
    "annotate_training_set = True    # set to True if you want training set points labeled with ligand ID\n",
    "\n",
    "model_sel = results.loc[model_number,\"Model\"]\n",
    "\n",
    "selected_feats = [X_labels.index(i) for i in models[model_sel].terms]\n",
    "X_train_sel = X_train[:,selected_feats]\n",
    "X_test_sel = X_test[:,selected_feats]\n",
    "lr = LogisticRegression().fit(X_train_sel,y_train)\n",
    "y_pred_train = lr.predict(X_train_sel)\n",
    "if len(VS) > 0:  \n",
    "    test_accuracy = test_accuracy_score(X_test_sel, y_test, X_train_sel, y_train)  \n",
    "kfold_score, kfold_stdev = kfold_logreg(X_train_sel, y_train, k=5)\n",
    "precision, recall, f1 = precision_recall_f1_score(X_train_sel, y_train)\n",
    "train_R2, test_R2 = calc_mcfadden_R2(X_train_sel, y_train, X_test_sel, y_test)\n",
    "\n",
    "print(\"Parameters and coefficients:\\n{:10.4f} + \\n\".format(lr.intercept_[0]) + \"\\n\".join([\"{:10.4f} * {}\".format(lr.coef_[0][i],X_labelname[selected_feats[i]]) for i in range(len(selected_feats))]))\n",
    "print(f\"\\nMcFadden Training R2  = {train_R2 :.2f}\")\n",
    "if len(VS) > 0:\n",
    "    print(f\"McFadden Test R2  = {test_R2 :.2f}\")\n",
    "print(f\"\\nAccuracy  = {100 * lr.score(X_train_sel, y_train):.1f} %\")\n",
    "if len(VS) > 0:\n",
    "    print(f\"Test Accuracy  = {100 * lr.score(X_test_sel, y_test):.1f}%\")\n",
    "print(\"\\nTraining K-fold Accuracy = {:.2f} (+/- {:.2f}) %\".format(100* kfold_score, 100* kfold_stdev ** 2))\n",
    "print(f\"f1 Score  = {f1:.3f}\")\n",
    "print(f\"Precision Score  = {precision :.3f}\")\n",
    "print(f\"Recall Score  = {recall:.3f}\")\n",
    "print('\\nNote: \\n(1) Black and white points denote active and inactive ligands respectively.')\n",
    "print('(2) Red and blue denote active and inactive chemical space respectively.')\n",
    "print('(3) Dashed lines denote 25% and 75% probability that a ligand will be active.')\n",
    "print('(4) Solid black line denotes 50% probability that a ligand will be active.')\n",
    "\n",
    "\n",
    "heatmap_logreg(selected_feats[0],selected_feats[1],df_train,df_test,results.iloc[model_number][0], \n",
    "               annotate_test=annotate_test_set, annotate_train=annotate_training_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089e2b9c",
   "metadata": {},
   "source": [
    "### Manual selection of features\n",
    "\n",
    "- Runs 2D logistic regression with user-defined parameters\n",
    "- populate features_x, annotate_test_set, annotate_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ddb95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_x = ('x13', 'x28') # tuple of features. eg ('x1', 'x10')\n",
    "annotate_test_set = False    # set to True if you want to test set points labeled with ligand id \n",
    "annotate_training_set = False    # set to True if you want to training set points labeled with ligand id \n",
    "\n",
    "\n",
    "selected_feats = sorted([X_labels.index(i.strip()) for i in features_x])\n",
    "X_train_sel = X_train[:,selected_feats]\n",
    "X_test_sel = X_test[:,selected_feats]\n",
    "lr = LogisticRegression().fit(X_train_sel,y_train)\n",
    "y_pred_train = lr.predict(X_train_sel)\n",
    "if len(VS) > 0:  \n",
    "    test_accuracy = test_accuracy_score(X_test_sel, y_test, X_train_sel, y_train)  \n",
    "kfold_score, kfold_stdev = kfold_logreg(X_train_sel, y_train, k=5)\n",
    "precision, recall, f1 = precision_recall_f1_score(X_train_sel, y_train)\n",
    "train_R2, test_R2 = calc_mcfadden_R2(X_train_sel, y_train, X_test_sel, y_test)\n",
    "\n",
    "print(\"Parameters and coefficients:\\n{:10.4f} + \\n\".format(lr.intercept_[0]) + \"\\n\".join([\"{:10.4f} * {}\".format(lr.coef_[0][i],X_labelname[selected_feats[i]]) for i in range(len(selected_feats))]))\n",
    "print(f\"\\nMcFadden Training R2  = {train_R2 :.2f}\")\n",
    "if len(VS) > 0:\n",
    "    print(f\"McFadden Test R2  = {test_R2 :.2f}\")\n",
    "print(f\"\\nAccuracy  = {100 * lr.score(X_train_sel, y_train):.1f} %\")\n",
    "if len(VS) > 0:\n",
    "    print(f\"Test Accuracy  = {100 * lr.score(X_test_sel, y_test):.1f}%\")\n",
    "print(\"\\nTraining K-fold Accuracy = {:.2f} (+/- {:.2f}) %\".format(100* kfold_score, 100* kfold_stdev ** 2))\n",
    "print(f\"f1 Score  = {f1:.3f}\")\n",
    "print(f\"Precision Score  = {precision :.3f}\")\n",
    "print(f\"Recall Score  = {recall:.3f}\")\n",
    "print('\\nNote: \\n(1) Black and white points denote active and inactive ligands respectively.')\n",
    "print('(2) Red and blue denote active and inactive chemical space respectively.')\n",
    "print('(3) Dashed lines denote 25% and 75% probability that a ligand will be active.')\n",
    "print('(4) Solid black line denotes 50% probability that a ligand will be active.')\n",
    "\n",
    "\n",
    "heatmap_logreg(selected_feats[0],selected_feats[1],df_train,df_test,features_x, annotate_test=annotate_test_set, annotate_train=annotate_training_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9be418",
   "metadata": {},
   "source": [
    "## 3.1.1 Virtual screening\n",
    "\n",
    "### Setup\n",
    "\n",
    "- Re-makes descriptors dataframe and combines training and test sets\n",
    "- Check for your model; if LOOCV approach, check for most common descriptors and match model number from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e7fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = pd.read_excel(comp_file+'.xlsx', comp_sheet,index_col=0,header=1,engine='openpyxl')\n",
    "X_all_names = np.array(ci.SMILES)\n",
    "compinp = ci[ci.columns[1:]].loc[ci.index[:]]\n",
    "compinp.dropna(axis=0,inplace=True)\n",
    "X_all = np.array(compinp)\n",
    "X_all_ids = np.array(compinp.index)\n",
    "\n",
    "# add all of the results to the training set\n",
    "print('Number of samples in original training set = ',len(y_train))\n",
    "y_train = np.concatenate((y_train,y_test), axis=0)\n",
    "X_train = np.concatenate((X_train,X_test), axis=0)\n",
    "print('Number of samples in updated training set = ',len(y_train))\n",
    "\n",
    "X_screen = X_all\n",
    "X_ids = X_all_ids.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40c4a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_model = 0 # index of model you want to use from results1 (int)\n",
    "model_sel = results.loc[use_model,\"Model\"] # choose the model\n",
    "\n",
    "# collect the model features for the training and screening sets \n",
    "selected_feats = [X_labels.index(i) for i in models[model_sel].terms]\n",
    "X_train_sel = X_train[:,selected_feats]\n",
    "X_screen_sel = X_screen[:,selected_feats]\n",
    "\n",
    "# perform lr on training set, then use model to predict training set (for R2) and screening set - gives points on the line\n",
    "lr = LogisticRegression().fit(X_train_sel,y_train)\n",
    "y_pred_train = lr.predict_proba(X_train_sel)[:,1]\n",
    "y_pred_screen =  lr.predict_proba(X_screen_sel)[:,1]\n",
    "\n",
    "# Pull McFadden R2 \n",
    "train_R2, test_R2 = calc_mcfadden_R2(X_train_sel, y_train, X_test_sel, y_test) # note that test_R2 here is a meaningless number\n",
    "print(f\"\\nMcFadden R2  = {train_R2 :.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8130cdc7",
   "metadata": {},
   "source": [
    "### Create table of virtual screening results with the visualized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = []\n",
    "for i in range(0,len(X_ids)):\n",
    "    j = str(X_all_names[i]) + \" (ID \" + X_ids[i] + \")\"\n",
    "    X_combined.append(j)\n",
    "\n",
    "df_virtual = pd.DataFrame(list(zip(X_combined, y_pred_screen)), columns = ['Alkyne','Predicted_probability']).sort_values('Predicted_probability', ascending=False)\n",
    "df_prediction = df_virtual.style.set_table_styles([dict(selector='th', props=[('text-align', 'center')])])\n",
    "df_prediction.set_properties(**{'text-align': 'center'}).hide_index()\n",
    "df_virtual.to_excel('Predictions.xlsx', index = True)\n",
    "df_virtual.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a5248",
   "metadata": {},
   "source": [
    "## 3.2. Leave-One-Out CV Approach\n",
    "- For small dataset (<50 samples), Uses Bootstrapping\n",
    "- df_combined from single descriptor logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb88cf",
   "metadata": {},
   "source": [
    "### Adapted Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1185223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_logreg_loo(X, y, feat_idx, X_names, colormap_scheme='RdBu', point_size=200):\n",
    "    \"\"\"\n",
    "    X: numpy array with shape (n_samples, n_features)\n",
    "    y: labels (0/1)\n",
    "    feat_idx: list of 2 indices of selected features\n",
    "    X_names: list of feature names (for axes)\n",
    "    \"\"\"\n",
    "    # Extract selected features\n",
    "    X_pair = X[:, feat_idx]\n",
    "    \n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_pair)\n",
    "    \n",
    "    # Fit logistic regression\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X_scaled, y)\n",
    "    \n",
    "    intercept = lr.intercept_[0]\n",
    "    c1, c2 = lr.coef_[0]\n",
    "    \n",
    "    # Create grid for heatmap\n",
    "    xx = np.linspace(X_scaled[:,0].min()-0.5, X_scaled[:,0].max()+0.5, 500)\n",
    "    yy = np.linspace(X_scaled[:,1].min()-0.5, X_scaled[:,1].max()+0.5, 500)\n",
    "    XX, YY = np.meshgrid(xx, yy)\n",
    "    grid = np.c_[XX.ravel(), YY.ravel()]\n",
    "    probs = lr.predict_proba(grid)[:,1].reshape(XX.shape)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    fig, ax = plt.subplots(figsize=(7.25,6))\n",
    "    heatmap = ax.imshow(probs, origin='lower',\n",
    "                        extent=[xx.min(), xx.max(), yy.min(), yy.max()],\n",
    "                        cmap=colormap_scheme, alpha=0.5, aspect='auto')\n",
    "    \n",
    "    # Decision boundary and 25/75% probability lines\n",
    "    x_vals = np.linspace(xx.min(), xx.max(), 500)\n",
    "    y_dec = -(intercept + c1*x_vals)/c2\n",
    "    y_75 = (np.log(3) - intercept - c1*x_vals)/c2\n",
    "    y_25 = (np.log(1/3) - intercept - c1*x_vals)/c2\n",
    "    ax.plot(x_vals, y_dec, color='black')\n",
    "    ax.plot(x_vals, y_75, color='black', linestyle='--')\n",
    "    ax.plot(x_vals, y_25, color='black', linestyle='--')\n",
    "    \n",
    "    # Overlay points with black and white circles\n",
    "    for label, facecolor in zip([0,1], ['white', 'black']):\n",
    "        ax.scatter(X_scaled[y==label,0], X_scaled[y==label,1],\n",
    "                   facecolors=facecolor, edgecolors='black', \n",
    "                   marker='o', s=point_size, label=f\"Class {label}\")\n",
    "    \n",
    "    # Labels and style\n",
    "    ax.set_xlabel(X_names[feat_idx[0]], fontsize=18)\n",
    "    ax.set_ylabel(X_names[feat_idx[1]], fontsize=18)\n",
    "    ax.tick_params(length=12, width=3)\n",
    "    for spine in ['top','bottom','left','right']:\n",
    "        ax.spines[spine].set_linewidth(3)\n",
    "    ax.xaxis.set_major_locator(plt.MaxNLocator(5))\n",
    "    ax.yaxis.set_major_locator(plt.MaxNLocator(4))\n",
    "    \n",
    "    cbar = plt.colorbar(heatmap, ax=ax)\n",
    "    cbar.set_label('Probability', size=18)\n",
    "    cbar.ax.tick_params(labelsize=15)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.savefig('Logistic_Regression_LOO.tif', dpi=600)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f04ba2",
   "metadata": {},
   "source": [
    "### Determine best two feature regressions for every single fold in an LOO loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea258503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 0. Imports\n",
    "# ============================================================\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, matthews_corrcoef\n",
    "from collections import Counter\n",
    "\n",
    "# ============================================================\n",
    "# 1. Helper: Bootstrap Confidence Interval\n",
    "# ============================================================\n",
    "def bootstrap_CI(y_true, y_pred, metric_fn, n_bootstrap=1000, random_state=42):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    stats = []\n",
    "    n = len(y_true)\n",
    "    for _ in range(n_bootstrap):\n",
    "        idx = rng.choice(n, n, replace=True)\n",
    "        stats.append(metric_fn(y_true[idx], y_pred[idx]))\n",
    "    return np.percentile(stats, [2.5, 97.5])\n",
    "\n",
    "# ============================================================\n",
    "# 2. Nested LOOCV using ForwardStep_py for 1-2 parameter models\n",
    "# ============================================================\n",
    "def nested_LOOCV_ForwardStep_with_CI(X, y, feature_names, verbose=False):\n",
    "    loo_outer = LeaveOneOut()\n",
    "    y_true, y_pred = [], []\n",
    "    selected_features_per_fold = []\n",
    "\n",
    "    X_df = pd.DataFrame(X, columns=feature_names)\n",
    "    y_series = pd.Series(y)\n",
    "\n",
    "    for i, (train_idx, test_idx) in enumerate(loo_outer.split(X)):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nOuter Fold {i+1}/{len(y)}\")\n",
    "\n",
    "        # ===============================\n",
    "        # Standardize features based on train\n",
    "        # ===============================\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        X_train_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n",
    "        X_test_df = pd.DataFrame(X_test_scaled, columns=feature_names)\n",
    "        y_train_series = pd.Series(y_train)\n",
    "\n",
    "\n",
    "        # Forward stepwise selection using the first script\n",
    "        results_fs, models_fs, _, sortedmodels, candidates = fsc.ForwardStep_py(\n",
    "            data=pd.concat([X_train_df, y_train_series.rename(\"y\")], axis=1),\n",
    "            response=\"y\",\n",
    "            reg=LogisticRegression(max_iter=1000),\n",
    "            collin_criteria=0.5\n",
    "        )\n",
    "\n",
    "        # Select the top model (highest accuracy)\n",
    "        top_model_terms = results_fs[\"Model\"].iloc[0]\n",
    "        selected_features_per_fold.append([feature_names.index(f) for f in top_model_terms])\n",
    "\n",
    "        # Fit logistic regression on selected features\n",
    "        lr = LogisticRegression(max_iter=1000)\n",
    "        lr.fit(X_train[:, [feature_names.index(f) for f in top_model_terms]], y_train)\n",
    "        y_hat = lr.predict(X_test[:, [feature_names.index(f) for f in top_model_terms]])[0]\n",
    "\n",
    "        y_true.append(y_test[0])\n",
    "        y_pred.append(y_hat)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Selected: {top_model_terms}, Accuracy={results_fs['Accuracy'].iloc[0]:.2f}\")\n",
    "\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Balanced Accuracy\": balanced_accuracy_score(y_true, y_pred),\n",
    "        \"F1\": f1_score(y_true, y_pred),\n",
    "        \"MCC\": matthews_corrcoef(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "    # Compute 95% CIs using bootstrap\n",
    "    CIs = {\n",
    "        \"Accuracy\": bootstrap_CI(y_true, y_pred, accuracy_score),\n",
    "        \"Balanced Accuracy\": bootstrap_CI(y_true, y_pred, balanced_accuracy_score),\n",
    "        \"F1\": bootstrap_CI(y_true, y_pred, f1_score),\n",
    "        \"MCC\": bootstrap_CI(y_true, y_pred, matthews_corrcoef)\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"selected_features_per_fold\": selected_features_per_fold,\n",
    "        \"metrics\": metrics,\n",
    "        \"CIs\": CIs\n",
    "    }\n",
    "\n",
    "# ============================================================\n",
    "# 3. Run Nested LOOCV with Bootstrap CIs\n",
    "# ============================================================\n",
    "results = nested_LOOCV_ForwardStep_with_CI(X, y_class, X_labels, verbose=True)\n",
    "\n",
    "# ============================================================\n",
    "# 4. Print Metrics with 95% Confidence Intervals\n",
    "# ============================================================\n",
    "print(\"\\n==============================\")\n",
    "print(\" Final Nested LOOCV Performance with 95% CIs\")\n",
    "print(\"==============================\")\n",
    "for m, v in results[\"metrics\"].items():\n",
    "    ci_low, ci_high = results[\"CIs\"][m]\n",
    "    print(f\"{m:20s}: {100*v:6.2f}% (95% CI: {100*ci_low:5.2f}-{100*ci_high:5.2f}%)\")\n",
    "\n",
    "# ============================================================\n",
    "# 5. Feature Selection Frequencies\n",
    "# ============================================================\n",
    "feat_counts = Counter(sum(results[\"selected_features_per_fold\"], []))\n",
    "freq_df = pd.DataFrame({\n",
    "    \"Feature\": [X_labels[i] for i in feat_counts.keys()],\n",
    "    \"Count\": list(feat_counts.values())\n",
    "}).sort_values(\"Count\", ascending=False)\n",
    "freq_df[\"Frequency (%)\"] = 100 * freq_df[\"Count\"] / len(results[\"selected_features_per_fold\"])\n",
    "print(\"\\n==============================\")\n",
    "print(\" Feature Selection Frequencies\")\n",
    "print(\"==============================\")\n",
    "print(freq_df)\n",
    "\n",
    "# ============================================================\n",
    "# 6. Co-selection Heatmap\n",
    "# ============================================================\n",
    "n_features = len(X_labels)\n",
    "co_matrix = np.zeros((n_features, n_features))\n",
    "for fold_feats in results[\"selected_features_per_fold\"]:\n",
    "    for i in range(len(fold_feats)):\n",
    "        for j in range(len(fold_feats)):\n",
    "            co_matrix[fold_feats[i], fold_feats[j]] += 1\n",
    "co_matrix /= len(results[\"selected_features_per_fold\"])\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(co_matrix, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, ax = plt.subplots(figsize=(20, 15))\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    ax.spines[axis].set_linewidth(3)\n",
    "ax.tick_params(length=12,width=3)\n",
    "ax.tick_params(which='minor', length=6, width=3)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(co_matrix, cmap=\"coolwarm\", mask=mask, xticklabels=X_names, yticklabels=X_names, linewidths=.5)\n",
    "cbar = ax.collections[0].colorbar\n",
    "# here set the labelsize by 20\n",
    "cbar.ax.tick_params(labelsize=15)\n",
    "\n",
    "fig.suptitle('Feature Co-Selection Frequency (1–2 Feature Models, LOOCV)', fontsize=25)\n",
    "plt.tight_layout()\n",
    "fig.savefig('Co-Selection.tif', dpi=600)\n",
    "plt.show()\n",
    "\n",
    "# ============================================================\n",
    "# 7. Decision Surface for Most Frequent Feature Pair\n",
    "# ============================================================\n",
    "pair_counts = Counter(tuple(sorted(f)) for f in results[\"selected_features_per_fold\"] if len(f) == 2)\n",
    "if len(pair_counts) > 0:\n",
    "    top_pair = pair_counts.most_common(1)[0][0]\n",
    "    feat1, feat2 = top_pair\n",
    "    print(f\"\\nMost frequently selected pair: {X_labels[feat1]} & {X_labels[feat2]}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a7ead1",
   "metadata": {},
   "source": [
    "### Analyze model performance for any two features and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79332fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, matthews_corrcoef\n",
    "import numpy as np\n",
    "\n",
    "# ===============================\n",
    "# Define features\n",
    "# ===============================\n",
    "feat_names = ['x13', 'x28']\n",
    "feat_idx = [X_labels.index(f) for f in feat_names]\n",
    "\n",
    "X_selected = X[:, feat_idx]\n",
    "\n",
    "# ===============================\n",
    "# LOOCV\n",
    "# ===============================\n",
    "loo = LeaveOneOut()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "for train_idx, test_idx in loo.split(X_selected):\n",
    "    X_train, X_test = X_selected[train_idx], X_selected[test_idx]\n",
    "    y_train, y_test = y_class[train_idx], y_class[test_idx]\n",
    "\n",
    "    # Standard scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Fit logistic regression\n",
    "    lr = LogisticRegression(max_iter=1000)\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_hat = lr.predict(X_test_scaled)[0]\n",
    "    y_true.append(y_test[0])\n",
    "    y_pred.append(y_hat)\n",
    "\n",
    "y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "\n",
    "# ===============================\n",
    "# Metrics\n",
    "# ===============================\n",
    "metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "    \"Balanced Accuracy\": balanced_accuracy_score(y_true, y_pred),\n",
    "    \"F1\": f1_score(y_true, y_pred),\n",
    "    \"MCC\": matthews_corrcoef(y_true, y_pred)\n",
    "}\n",
    "\n",
    "CIs = {m: bootstrap_CI(y_true, y_pred, fn) for m, fn in \n",
    "       zip(metrics.keys(), [accuracy_score, balanced_accuracy_score, f1_score, matthews_corrcoef])}\n",
    "# ===============================\n",
    "# Print results\n",
    "# ===============================\n",
    "print(\"\\n==============================\")\n",
    "print(f\"LOOCV Performance for {X_names[feat_idx[0]]} and {X_names[feat_idx[1]]} with 95% CI\")\n",
    "print(\"==============================\")\n",
    "for m, v in metrics.items():\n",
    "    ci_low, ci_high = CIs[m]\n",
    "    print(f\"{m:20s}: {100*v:6.2f}% (95% CI: {100*ci_low:5.2f}-{100*ci_high:5.2f}%)\")\n",
    "\n",
    "heatmap_logreg_loo(X, y_class, feat_idx, X_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
